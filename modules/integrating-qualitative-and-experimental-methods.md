---
layout: page
title: Integrating Qualitative and Experimental Methods (Modules 17, 21, 25)
---

Monday, June 23; Tuesday, June 24, Wednesday, June 25
{: #module-date}

**Chris Carter (University of Virginia), Guadalupe Tuñón (Princeton University), Tesalia Rizzo Reyes (UC Merced), Charles Crabtree (Dartmouth College)**

In this module sequence, we introduce natural and randomized experiments and discuss their strengths and limitations through a survey of recent examples from political science and economics. We introduce a common framework for understanding and assessing natural and randomized experiments based on the credibility of causal and statistical assumptions. We discuss tools for developing and accessing experimental designs, such as instrumental variable analysis, sampling principles, power analysis, data collection do’s and don’ts as well as a variety of robustness tests. We then discuss how to bolster the credibility of natural and randomized experiments in the design stage. We will focus on the role of “ex-ante” approaches to improve the quality and transparency of research designs, such as the use of pre-analysis plans. The module incorporates applied research and practical advice, especially on how to conduct fieldwork, collect data, and analyze the logistics and ethics surrounding experiments. We end the module by evaluating the promise and obstacles to the use of multi-method research in the analysis of natural and randomized experiments. We discuss how qualitative methods can help address some of the criticisms of experiments, as well as how experiments can bolster the inferences drawn from qualitative evidence.

***Participants may enter the module sequence after it has begun.***

*There is online content associated with this module that students should view (and complete the assignments for) prior to June 23*

## Integrating Qualitative and Experimental Methods (I) (M17, June 23)
{: .module-sub }

### 8:45am - 10:15am – Qualitative approaches to natural experiments

**Carter and Tuñón**

We emphasize the crucial role of qualitative methods in analyzing natural experiments. Through illustrative examples, we demonstrate how qualitative evidence enhances the credibility of causal assumptions and aids in interpreting quantitative results. We also explore how qualitative methods address common criticisms of natural experiments and how natural experiments, in turn, strengthen inferences drawn from qualitative evidence.

**Required Reading:**

  - Dunning, T. (2012). *Natural experiments in the social sciences: A design-based approach*. Cambridge University Press. Chapter 1, pp. 105-121, and Chapter 7. (Book to obtain, ebook pdf is available at SU library)

  - Kocher, M.A. and Monteiro, N.P. (2016). “Lines of Demarcation: Causation, Design Based Inference, and Historical Research.” *Perspectives on Politics*. 14 (4): 952-975.

Suggested Reading:

  - Ferwerda, J. & Miller, N. (2014). “Political Devolution and Resistance to Foreign Rule: A Natural Experiment.” *American Political Science Review*. 108(3), 642-660.

  - Jeremy Ferwerda and Nicholas Miller. (2015). “Rail Lines and Demarcation Lines: A Response”

  - Gerber, A. S., & Green, D. P. (2008). “Field experiments and natural experiments.” In *The Oxford Handbook of Political Science*.

  - Rosenbaum, P. (2010). *Design of Observational Studies*. Springer. Chapter 3. (ebook pdf is also available at SU library)

### 1:30pm - 3:00pm – Multi-method approaches to Instrumental Variables (IV) analysis

**Carter and Tuñón**

In this session, we discuss the role of causal and statistical assumptions in the analysis of natural experiments. We then delve deeper into one of the most widely used types of natural experiments—instrumental variables (IV)— to illustrate the plausibility of these assumptions in a variety of applications. We discuss qualitative methods that can be used to discover, strengthen, and assess the validity of instrumental variables. 

**Required readings:**

  - Dunning, T. (2012). *Natural experiments in the social sciences: A design-based approach*. Cambridge University Press. Chapter 4 and pp. 135-153. (Book to obtain, ebook pdf is available at SU library)

Suggested Readings:

  - Clingingsmith, D., Khwaja, A. I., & Kremer, M. (2009). Estimating the impact of the Hajj: religion and tolerance in Islam's global gathering. *The Quarterly Journal of Economics*, 124(3), 1133-1170.

  - Di Tella, R., Galiant, S., & Schargrodsky, E. (2007). “The formation of beliefs: evidence from the allocation of land titles to squatters.” *The Quarterly Journal of Economics*, 122(1), 209-241.

### 3:30pm - 5:00pm – Multi-method approaches to Regression-Discontinuity (RD) Designs

**Carter and Tuñón**

In this session we focus on a second broad category of natural experiments, regression-discontinuity (RD) designs. We provide an overview of core assumptions, discussing the role of qualitative and quantitative methods in understanding the process that determines treatment assignment. We discuss recent applications of RD designs and how they might be bolstered by additional qualitative evidence.

**Required readings:**

  - Dunning, T. (2012). *Natural experiments in the social sciences: A design-based approach*. Cambridge University Press. Chapter 3. (Book to obtain, ebook pdf is available at SU library)

  - Hinnerich, B. T., & Pettersson-Lidbom, P. (2014). “Democracy, redistribution, and political participation: Evidence from Sweden 1919–1938.” *Econometrica*, 82(3), 961-993.

Suggested Readings:

  - Posner, D. N. (2004). The political salience of cultural difference: Why Chewas and Tumbukas are allies in Zambia and adversaries in Malawi. *American Political Science Review*, 98(4), 529-545.


## Integrating Qualitative and Experimental Methods (II) (M21 June 24)
{: .module-sub }

### 8:45am - 10:15am – Designing natural experiments: Applications and Placebo Tests

**Carter and Tuñón**

We offer a review of the core approaches to natural experiments, working through a guided review of a problem set (to be completed before class). We then discuss how students might identify and evaluate natural experiments in their own research, as well as when evaluating research conducted by others.

In lieu of required readings, students should complete the assigned problem set for this session.

Suggested readings:

  - Sekhon, J. S., & Titiunik, R. (2012). “When natural experiments are neither natural nor experiments.” American Political Science Review, 106(1), 35-57.

  - Eggers, A. C., Tuñón, G., & Dafoe, A. (2024). Placebo tests for causal inference. *American Journal of Political Science*, *68*(3), 1106-1121.

### 1:30pm - 3:00pm – Nuts and Bolts of Implementing Randomized Experiments: Practical Considerations and Mixed-Methods Approaches

**Rizzo**

This session offers a review of the fundamental components of experimental design, including selection bias, randomization, and sampling strategies. We will also introduce different causal estimands, estimation strategies, and common threats to causal inference. This section assumes prior exposure to basic causal inference concepts and is intended as a refresher.

In the second half, we explore the comparative advantages of field experiments over other empirical strategies. Particular attention will be given to ongoing debates over external validity and the robustness of experimental findings. We conclude by discussing how qualitative methods can enhance experimental design—particularly in the development of treatments and the identification of mechanisms (a topic we will return to more in depth in the next session).

**Required Readings – External Validity**

  - Findley, M. G., Kikuta, K., & Denly, M. J. (2021). External validity. *Annual Review of Political Science*, 24, 365–393.

**Required Readings – Integrating Qualitative Methods**

  - Paluck, E. L. (2010). The promising integration of qualitative methods and field experiments. *The ANNALS of the American Academy of Political and Social Science*, 628(1), 59–71.

  - Rao, V. (2023). Can economics become more reflexive? Exploring the potential of mixed methods. In *Handbook on economics of discrimination and affirmative action* (pp. 323-349). Singapore: Springer Nature Singapore.

Suggested Reading:

  - *If you need a refresher on the basics consult the following:*
    
      - Imai, K. (2018). *Quantitative social science: An introduction*. Princeton University Press. Sections 2.1–2.4, pp. 34–54.
    
      - EGAP. (n.d.). *10 Things to Know About Statistical Power*. & EGAP. (n.d.). *10 Things to Know About Pre-Analysis Plans*.
    
      - Gerber, A. S., & Green, D. P. (2012). Chapter 2: Causal Inference and Experimentation and Chapter 3: Sampling distributions, statistical inference, and hypothesis testing.
    
      - Glennerster, R. and Takavarasha, K. (2013). *Running randomized evaluations: A practical guide*. Princeton University Press, Chapter 6: Statistical Power.

  - *On the history of RCTs:*
    
      - Gueron, J. M. (2017). The politics and practice of social experiments: Seeds of a revolution. In A. Banerjee & E. Duflo (Eds.), *Handbook of economic field experiments* (Vol. 1, pp. 27–69). North-Holland.

  - *Additional readings on external validity*
    
      - Egami, N., & Hartman, E. (2023). Elements of external validity: Framework, design, and analysis. *American Political Science Review*, 117(3), 1070–1088.
    
      - Wilke, A., & Humphreys, M. (2020). Field experiments, theory, and external validity. In L. Curini & R. Franzese (Eds.), *SAGE Handbook of Research Methods in Political Science and International Relations* (pp. 1007–1035). SAGE.
    
      - Slough, T., & Tyson, S. A. (2023). External Validity and Meta‐Analysis. *American Journal of Political Science*, *67*(2), 440-455.
    
      - Saccardo, S., Dai, H., Han, M. A., Vangala, S., Hoo, J., & Fujimoto, J. (2024). Field testing the transferability of behavioural science knowledge on promoting vaccinations. *Nature Human Behaviour*, 8(5), 878–890.

  - *Additional readings on integrating qualitative and quantitative methods*
    
      - Gerring, J. (2017). Qualitative methods. *Annual review of political science*, *20*(1), 15-36.
    
      - Blattman, C., Jamison, J., Koroknay-Palicz, T., Rodrigues, K., & Sheridan, M. (2016). Measuring the measurement error: A method to qualitatively validate survey data. *Journal of Development Economics*, *120*, 99-112.
    
      - Small, M. L. (2011). How to conduct a mixed methods study: Recent trends in a rapidly growing literature. *Annual review of sociology*, *37*(1), 57-86.
    
      - Bussell, J. (2020). Shadowing as a tool for studying political elites. *Political Analysis*, *28*(4), 469-486.
    
      - *Something interesting from public health:* O'Cathain, A. (2018). *A practical guide to using qualitative research with randomized controlled trials*. Oxford University Press.

### 3:30pm - 5:00pm – Nuts and Bolts of Implementing Randomized Experiments: Practical Considerations and Mixed-Methods Approaches

**Rizzo**

This session focuses on the practicalities of *implementing* field experiments. In the first half, we examine strategies for data collection, including survey design, enumerator training, quality control, and ensuring treatment compliance. Emphasis is placed on using qualitative methods—such as interviews, focus groups, and participant observation—to limit implementation issues and develop locally grounded treatments and minimize measurement error.

In the second half, we discuss how to collaborate effectively with implementation partners (e.g., NGOs, survey firms, government partners, etc.). We will especially focus on how qualitative insights from fieldwork can help anticipate logistical challenges, strengthen enumerator training, and improve collaboration with relevant actors

**Required Readings:**

  - Innovations for Poverty Action (IPA). *Research Protocols*.: https://poverty-action.org/research-protocols

  - EGAP. (n.d.). *Ten Things to Know About Survey Design*.https://egap.org/resource/10-things-to-know-about-survey-design/

  - EGAP. (n.d.). *Ten Things to Know About Survey Implementation: https://methods.egap.org/guides/implementation/survey-implementation\_en.html*

  - Nickerson, D. W., & Hyde, S. D. (2016). Conducting research with NGOs: Relevant counterfactuals from the perspective of subjects. In S. Desposato (Ed.), *Ethics and experiments* (pp. 198–216). Routledge.

  - Thachil, T. (2018). Improving surveys through ethnography: Insights from India’s urban periphery. *Studies in Comparative International Development*, 53(3), 281–299.

## Integrating Qualitative and Experimental Methods (III) (M25 June 25) 
{: .module-sub }

### 8:45am - 10:15am – Discussion of Recent Field Experiments: Ethics, Research Transparency, and the Role of Randomized Experiments in Social Science

**Rizzo**

In this session, we will examine key ethical considerations in conducting field experiments in political science through the lens of recent studies. Topics will include the ethics of randomizing (and withholding) treatments, potential interference, power dynamics in the field, and ensuring the safety of respondents and field staff. We will also explore recent debates on positionality in research, particularly when working with vulnerable populations, and how researcher identity and power relations shape fieldwork dynamics.

Building on these ethical discussions, we will consider best practices in research transparency, including pre-registration, ethical reporting, and replication, to ensure rigor and accountability in experimental research.

The case studies will not be discussed in detail. Read them enough to be familiar with these examples' general idea, design, and implementation.

**Required Readings - Case Studies for Class Discussion (Skim them, focus on the research designs, treatments, and implementation).**

  - Alsan, M., Garrick, O., & Graziani, G. (2019). Does diversity matter for health? Experimental evidence from Oakland. *American Economic Review*, 109(12), 4071–4111.

  - Blair, R. A., Mendoza‐Mora, L., & Weintraub, M. (2025). Mano dura: An experimental evaluation of military policing in Cali, Colombia. *American Journal of Political Science*.

  - Gaikwad, N., Hanson, K., & Tóth, A. (2024). Bridging the gulf: How migration fosters tolerance, cosmopolitanism, and support for globalization. *American Journal of Political Science*.

  - He, G., Pan, Y., Park, A., Sawada, Y., & Tan, E. S. (2023). Reducing single-use cutlery with green nudges: Evidence from China’s food-delivery industry. *Science*, 381(6662).

**Required Readings – Ethics in Field Experiments and Fieldwork**

  - Phillips, T. (2021). Ethics of field experiments. *Annual Review of Political Science*, 24(1), 277–300.

  - Herman, B., Panin, A., Wellman, E. I., et al. (2022). Field experiments in the Global South: Assessing risks, localizing benefits, and addressing positionality. *PS: Political Science & Politics*, 55(4), 769–772.

Suggested Reading:

  - Cronin-Furman, K., & Lake, M. (2018). Ethics abroad: Fieldwork in fragile and violent contexts. *PS: Political Science & Politics*, 51(3), 607–614

  - Deaton, A., & Cartwright, N. (2018). Understanding and misunderstanding randomized controlled trials. *Social science & medicine*, *210*, 2-21.

  - Slough, T. (2024). Making a difference: The consequences of electoral experiments. *Political Analysis*, 32(4), 384–400.

  - Desposato, S. (2018). Subjects and scholars’ views on the ethics of political science field experiments. *Perspectives on Politics*, 16(3), 739–750.

  - Desposato, S. (Ed.). (2015). *Ethics and experiments: Problems and solutions for social scientists and policy professionals*. Routledge.

  - Humphreys, M. (2015). Reflections on the ethics of social experimentation. *Journal of Globalization and Development*, 6(1), 87–112.

  - Blair, G., Cooper, J., Coppock, A., & Humphreys, M. (2019). Declaring and diagnosing research designs. *American Political Science Review*, *113*(3), 838-859.

  - Humphreys, M., De la Sierra, R. S., & Van der Windt, P. (2013). Fishing, commitment, and communication: A proposal for comprehensive nonbinding research registration. *Political Analysis*, 21(1), 1–20.

Suggested Additional Cases:

  - Fishkin, J., Siu, A., Diamond, L., & Bradburn, N. (2021). Is deliberation an antidote to extreme partisan polarization? *American Political Science Review*, 115(4), 1464–1481.

  - Fishkin, J., Bolotnyy, V., Lerner, J., Siu, A., & Bradburn, N. (2024). Can deliberation have lasting effects? *American Political Science Review*, 118(4), 2000–2020.

  - Wilke, A. M. (2024). How the State Discourages Vigilantism—Evidence From a Field Experiment in South Africa. *American Journal of Political Science*.

  - Badrinathan S. (2021). Educative Interventions to Combat Misinformation: Evidence from a Field Experiment in India. *American Political Science Review*. 115(4):1325-1341.

  - Bhandari, A. (2022). Political determinants of economic exchange: evidence from a business experiment in Senegal. *American Journal of Political Science*, *66*(4), 835-852.

  - Cantoni, D., Yang, D. Y., Yuchtman, N., & Zhang, Y. J. (2019). Protests as strategic games: Experimental evidence from Hong Kong's anti-authoritarian movement. *Quarterly Journal of Economics*, 134(2), 1021–1077.
  

### 1:30pm - 3:00pm – Advanced Topics in Experiments: Ecological validity

**Crabtree**

In this session, we examine ecological validity in survey experiments,
focusing on how well experimental findings generalize to real-world
settings. We discuss how qualitative methods can help assess external
validity, uncover contextual factors that shape responses, and improve
study design. We review recent experimental work, explore ecological
limitations, and outline strategies and concrete recommendations for
strengthening ecological validity. To ground understanding of this
issue, we’ll work through a toy research design that examines race-based
discrimination.

**Required readings:**

  -  Trochim, W. M., & Donnelly, J. P. (2001). *Research methods
     knowledge base* (Vol. 3). Cincinnati, OH: Atomic dog publishing.
     Chapter 3.

  -  Kaufman, A., Celaya, C., & Grumbach, J. (2024). Improving
     Compliance in Experimental Studies of Discrimination. *Journal of
     Politics.*

  -  Crabtree, C., Holbein, J., & Gaddis, M. S. (2025). How Should
     Researchers Signal Race in Correspondence Audits? Considering The
     Experimental Validity of Names and Photographs. *Working paper*.

Suggested readings:

  -  Costello, T. H., Pennycook, G., & Rand, D. G. (2024). Durably
     reducing conspiracy beliefs through dialogues with AI. *Science*,
     *385*(6714), eadq1814.

  -  Brutger, R., Kertzer, J. D., Renshon, J., Tingley, D., & Weiss, C.
     M. (2023). Abstraction and detail in experimental design.
     *American Journal of Political Science*, *67*(4), 979-995.

  -  Druckman, J. N., & Green, D. P. (Eds.). (2021). *Advances in
     experimental political science*. Cambridge University Press.
     Selections.

  -  Kihlstrom, J. F. (2021). Ecological validity and “ecological
     validity”. *Perspectives on Psychological Science*, *16*(2),
     466-471.

  -  Westreich, D., Edwards, J. K., Lesko, C. R., Cole, S. R., &
     Stuart, E. A. (2019). Target validity and the hierarchy of study
     designs. *American journal of epidemiology*, *188*(2), 438-443.

### 3:30pm - 5:00pm – Advanced Topics in Experiments: Information Equivalence

**Crabtree**

In this session, we examine information equivalence in survey
experiments, focusing on how experimental treatments convey meaning to
respondents. We discuss challenges in ensuring treatments are understood
as intended and explore how qualitative methods can identify variation
in interpretation. We review recent experimental work, explore issues
related to information equivalence, and outline strategies and concrete
recommendations for designing treatments with high levels of construct
validity. To ground understanding of this issue, we’ll work through a
toy research design that examines gender-based discrimination.

**Required readings:**

  -  Trochim, W. M., & Donnelly, J. P. (2001). *Research methods
     knowledge base* (Vol. 3). Cincinnati, OH: Atomic dog publishing.
     Chapter 3.

  -  Dafoe, A., Zhang, B., & Caughey, D. (2018). Information
     equivalence in survey experiments. *Political Analysis*, *26*(4),
     399-416.

  -  Hou, Y., Liu, C., & Crabtree, C. (2020). Anti-Muslim bias in the
     Chinese labor market. *Journal of Comparative Economics*, *48*(2),
     235-250.

Suggested readings:

  -  Druckman, J. N., & Green, D. P. (Eds.). (2021). *Advances in
     experimental political science*. Cambridge University Press.
     Selections.

  -  Crabtree, C., Gaddis, S. M., Holbein, J. B., & Larsen, E. N.
     (2022). Racially distinctive names signal both race/ethnicity and
     social class. *Sociological Science*, *9*, 454-472.

  -  Adcock, R., & Collier, D. (2001). Measurement validity: A shared
     standard for qualitative and quantitative research. *American
     Political Science Review*, *95*(3), 529-546.
